{
  "project": "Claude Code Docs Crawler",
  "prd": "docs/ai/ralph/backlog/2026-02-01.docs-crawler/prd.2026-02-01.docs-crawler.md",
  "branchName": "ralph/docs-crawler",
  "description": "A Node.js CLI tool that crawls Claude Code docs from llms.txt, saving all in-scope content locally for offline access and searchable indexing",
  "userStories": [
    {
      "id": "US-001",
      "title": "Create project structure",
      "description": "As a developer, I need the basic project setup so I can run the crawler.",
      "acceptanceCriteria": [
        "package.json with \"type\": \"module\" and \"crawl\" script pointing to crawl.js",
        ".gitignore includes content/ directory",
        "lib/ directory created for modules",
        "Typecheck passes"
      ],
      "priority": 1,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-002",
      "title": "Implement fetch with redirect handling",
      "description": "As a developer, I need a fetch wrapper that handles redirects manually so I can control scope validation at each redirect hop.",
      "acceptanceCriteria": [
        "lib/fetch.js exports fetchWithRedirects(url, scopePrefix, maxRedirects)",
        "Follows redirects (3xx) only if target stays within scope",
        "Returns { type: 'out-of-scope', originalUrl, redirectedTo } for out-of-scope redirects",
        "Returns { type: 'rate-limited', retryAfter } for 429 responses",
        "Parses Retry-After header (seconds to milliseconds)",
        "Returns { type: 'error', reason | status } for failures",
        "Returns { type: 'success', finalUrl, body } for successful fetches",
        "Limits redirects to 10 hops maximum",
        "Typecheck passes"
      ],
      "priority": 2,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-003",
      "title": "Implement URL parsing from content",
      "description": "As a developer, I need to extract URLs from fetched content so the crawler can discover new pages.",
      "acceptanceCriteria": [
        "lib/parse.js exports parseUrls(body, baseUrl, scopePrefix)",
        "Extracts markdown links [text](url)",
        "Extracts markdown reference links [label]: url",
        "Extracts HTML href attributes href=\"url\"",
        "Extracts bare HTTPS URLs",
        "Resolves relative URLs against base URL",
        "Strips URL fragments (#section)",
        "Filters to only in-scope URLs",
        "Returns deduplicated array",
        "Typecheck passes"
      ],
      "priority": 3,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-004",
      "title": "Implement file saving",
      "description": "As a developer, I need to save fetched content to the filesystem in an organized structure.",
      "acceptanceCriteria": [
        "Saves to content/<host>/<path> structure",
        "Handles directory-style URLs (no extension) by saving as index.txt",
        "Creates parent directories recursively",
        "Logs saved file path to console",
        "Typecheck passes"
      ],
      "priority": 4,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-005",
      "title": "Implement crawl queue and state management",
      "description": "As a developer, I need queue management to track URLs to fetch and avoid duplicates.",
      "acceptanceCriteria": [
        "queue array for URLs waiting to be fetched",
        "queued Set for O(1) duplicate checking",
        "fetched Set for already-fetched URLs",
        "attempts Map tracking retry count per URL",
        "enqueue(url) normalizes and adds if not queued/fetched",
        "dequeue() removes and returns next URL",
        "requeue(url) adds URL to end of queue",
        "Typecheck passes"
      ],
      "priority": 5,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-006",
      "title": "Implement main crawl loop",
      "description": "As a developer, I need the main crawl loop that coordinates fetching, parsing, and saving.",
      "acceptanceCriteria": [
        "Seeds queue from https://code.claude.com/docs/llms.txt",
        "Processes queue until empty",
        "Tracks attempt count, max 3 per URL",
        "On success: saves content, parses for new URLs, enqueues discoveries",
        "On rate limit: waits for Retry-After (default 5s), requeues",
        "On error: logs and requeues (up to 3 attempts)",
        "On out-of-scope redirect: logs and skips (no requeue)",
        "Logs summary when complete",
        "Typecheck passes"
      ],
      "priority": 6,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-007",
      "title": "Implement 429 abort threshold",
      "description": "As a developer, I need the crawler to abort if rate-limited repeatedly so it doesn't run indefinitely.",
      "acceptanceCriteria": [
        "Tracks consecutive 429 responses (counter)",
        "Non-429 responses reset the counter",
        "Aborts crawl with error message at 3 consecutive 429s",
        "Exits with non-zero status code on abort",
        "Typecheck passes"
      ],
      "priority": 7,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-008",
      "title": "Generate metadata JSON file",
      "description": "As a developer, I need a metadata file to track crawl execution results for debugging and monitoring.",
      "acceptanceCriteria": [
        "Creates content/metadata.json after each crawl",
        "Includes seedUrl, scopePrefix, lastUpdate (ISO timestamp)",
        "Includes result: success, partial, or aborted",
        "Includes stats object with counts for all status categories",
        "Includes items object mapping paths to individual results",
        "Each item has status, statusReason, fetchedAt",
        "Typecheck passes"
      ],
      "priority": 8,
      "passes": false,
      "notes": ""
    }
  ]
}
